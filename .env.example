# LiteLLM Proxy Configuration (Preferred)
# If using LiteLLM proxy, set these:
LITELLM_BASE_URL=http://localhost:4000  # Your LiteLLM proxy URL
LITELLM_API_KEY=your-litellm-api-key
LITELLM_MODEL=gpt-4  # Model name configured in LiteLLM

# Azure OpenAI Configuration (Alternative)
# If using Azure OpenAI directly, set these:
AZURE_OPENAI_API_KEY=your-azure-openai-key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4  # Your deployment name
AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=gpt-4  # Chat model deployment

# OpenAI Configuration (For scanners that require it)
OPENAI_API_KEY=your-openai-key

# LLM Provider Selection
# Options: "litellm" or "azure"
LLM_PROVIDER=litellm

# Agent Configuration
AGENT_TEMPERATURE=0.7
AGENT_MAX_TOKENS=2000

# A2A Protocol Configuration
A2A_HOST=localhost
A2A_PORT=8000
A2A_BASE_URL=http://localhost:8000

# File Operations Agent Settings
FILE_AGENT_ALLOWED_PATHS=/tmp,/Users/aviral/Work/agent-redteaming/test_files
FILE_AGENT_MAX_FILE_SIZE=10485760  # 10MB in bytes

# Web Research Agent Settings
WEB_AGENT_MAX_RESULTS=5
WEB_AGENT_TIMEOUT=30

# Red-teaming Configuration
REDTEAM_OUTPUT_DIR=./reports
REDTEAM_LOG_LEVEL=INFO
